# Polynomial time computable problems

It's now time to take a look at what it means to solve a task in a **reasonable amount of time**: our notion of computability, even though it specifies the actual time which is needed, doesn't say much about what it means for such a function to bound the time to be small enough. We have also seen exercises having a time of computation which is linear: the function bounding the time is a linear function. 

Is it small enough? Too big? A time is reasonable if **polynomial**. We want the task to be computable with resource bounds: a reasonable amount of time and space. When the input string gets bigger, the amount of time grows, but not *too much*. Beware: when we work with a complexity class we are not talking about a set of machines, rather of tasks (functions/languages). Of course we can say that a machine works in polynomial time, though this doesn't mean that the machine is part of that class. We want to classify tasks, not machines. We'll work with tasks given by **decision problems**, i.e. we work with subsets of the set of all binary strings. 

A TM deciding the language runs in time <img src="svgs/4772fbb3859c73242a0d6c302b506869.svg?invert_in_darkmode" align=middle width=88.96488314999999pt height=24.65753399999998pt/>. The letter *D* in <img src="svgs/dc01ac8030dce255ab453d52e3f33d86.svg?invert_in_darkmode" align=middle width=65.29341885pt height=22.465723500000017pt/> refers to **determinism**: the machine works deterministically. These are the kind of machines we have already studied. They cannot *branch and decide*, i.e. working in non-deterministic ways. We could base our analysis on these classes.

Why do we need something robust?
Let's first define this robustness: we need a class which is not just in the form <img src="svgs/df8c6400530a893f8d53c4b9004d346b.svg?invert_in_darkmode" align=middle width=84.01493099999999pt height=24.65753399999998pt/>, but in the union of these classes <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/>, defined as the union on all the constants <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.11380504999999pt height=14.15524440000002pt/> of <img src="svgs/18e705ff53cffd07955ea45a072d17e0.svg?invert_in_darkmode" align=middle width=94.64227904999998pt height=24.65753399999998pt/>: a problem is in <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> if it can be solved by a TM working deterministically and taking polynomial time.
Why is that? The <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.11380504999999pt height=14.15524440000002pt/> is arbitrary, we take the union of all the possible <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.11380504999999pt height=14.15524440000002pt/>, i.e. the quadratic, cubic,... times. It is a rather big class, but as long as <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.11380504999999pt height=14.15524440000002pt/> is finite, it is still polynomial. 

So, why are we considering this class? One of the reasons lies in the ***Church-Turing thesis***. One may wonder why TM are an appropriate basis for computer complexity theory. Every computer can be simulated by a TM with an overhead in time, so the class of computable tasks would not be larger if formalized as so. This is a thesis: it's impossible to prove it for all possible architectures, but most scientists believe in it and nobody has still found a counterexample.
Then, there's a **strong** version of this, having to do with complexity rather than computability: we've said that the overhead may be very large, and in this thesis we impose the constraint that the actual overhead **must be polynomial**.
According to this thesis, the class <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> would be the **exact same**, provided that it holds. If it holds and a problem can be solved in polynomial time with any hardware, it can be solved in polynomial time with a TM. Of course the exponent will grow, though. This thesis is more controversial, for example due to quantum computation. 

Why polynomials? First of all, <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> is robust: they seem to be the smallest çlass of bounds which make <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> robust. Then, exponents are often quite small: we could object to the fact that they can be arbitrarily big. We may be right, but usually we get quadratic/cubic bounds. Finally, we can well say that the class <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> is closed under various operations on programs, in particular composition and bounded loops. Suppose that you have one problem which can be spelled out as the composition of 2 different problems, like solving the problem of multiplying two numbers by iterating additions. As a result, it is pretty easy to prove that a problem is in the class: it suffices to give an algorithm and prove that it solves in polynomial time.

There are downsides too. One of these is that it is based on worst cases: the definition of <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> says that there must be one single polynomial and one TM such that for every input... If for example, a problem requires linear time in most of the cases and exponential time in just one case, we cannot conclude that the problem is in <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/>.
This is a quite big problem. There are then alternative computational problems giving birth to other classes, like BPP or BQP. Finally, not all problems can be expressed as decision problems but just as functions. 

## FP class

We'd like to classify functions instead of languages. A function is in the class <img src="svgs/1c77ec476d0ffc061124a21e00d449ae.svg?invert_in_darkmode" align=middle width=78.14734124999998pt height=22.465723500000017pt/> if there is a TM computing <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.81741584999999pt height=22.831056599999986pt/> in time proportional to <img src="svgs/cc4152a1ba8a0ec113e9f2062a489b7d.svg?invert_in_darkmode" align=middle width=34.54162139999999pt height=24.65753399999998pt/> for some constant <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.11380504999999pt height=14.15524440000002pt/>. 
<p align="center"><img src="svgs/b9a145d3cc1682e57e3987faa1147e8f.svg?invert_in_darkmode" align=middle width=181.34142015pt height=39.0630438pt/></p>

Obviously, if we have a characteristic function <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.81741584999999pt height=22.831056599999986pt/> for a language which is in <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/>, <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.81741584999999pt height=22.831056599999986pt/> will be in <img src="svgs/83e7c965e79585945f597a03a99a0f43.svg?invert_in_darkmode" align=middle width=25.69069799999999pt height=22.465723500000017pt/>.

We can turn functions into languages (the inverse is obvious - is it?) in some canonical ways. 

Some examples of problems in this class might be introduced with **lists**, like scanning them, inverting them, sorting them...

## Proving a task being in P or FP

Now, how do we actually prove if a task is in P or FP? In most cases, we'll be allowed to use pseudocode describing an algorithm that actually solves the problem. This is much easier to write than a TM. We'll want to prove that the input can be encoded as a binary string, that all the instructions take polynomial time to be simulated.

## EXP

There is one last thing: a class bigger than <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/>, <img src="svgs/91523558357e27055dc14ed672b20165.svg?invert_in_darkmode" align=middle width=40.82761979999999pt height=22.465723500000017pt/>. This is a superset of <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.83677559999999pt height=22.465723500000017pt/> and contains all decision problems which are solvable in exponential time by a deterministic Touring Machine.

As _P_ has a similar class for non-deterministic TM, _NP_, also _EXP_ has a class like that: _NEXP_, which contains all decision problems which are solvable in exponential time by a non-deterministic Touring Machine.

#
[Previous section](2%20-%20The%20computational%20model.md) · [Next section](4%20-%20Between%20feasible%20and%20unfeasible.md)




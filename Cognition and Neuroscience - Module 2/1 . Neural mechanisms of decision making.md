# Neural mechanisms of decision-making

This is kind of more connected to what we do. It is indeed connected to reinforcement learning. 

**Decision Making** is the processof taking decisions: a **voluntary process**, which is very long (a decision could take long times) not instantaneous, making it easier to study. This process terminates when the subject chooses an action, which is able to *optimize reward* and reduce the amount of *punishment*. Decisions are made in the face of reward and punishment, which are known as **reinforcement**. 

So, this is all about action decision. Actually, it's a rather huge field: making a decision involves many different things, like perceiving visual alternatives, responses, actions, gathering information, internal states... States are not only external, but internal too. Decision involve practically the whole brain: memory, vision, hearing...

Decisions are **inherently stochastic**, probabilistic. The output is gotten from a probability distribution. Why is so? First of all, the *agent*, we, animals, are inconsistent. Decisions are not taken deterministically, and we can choose in a sub-optimal way. We aren't fully inconsistent though: we would be irrational, if so. Then, also, we don't know the outcomes in advance: we may know the probability of rewards/punishments, but that's all. No certain outcome. Also, the signal that we receive from vision are corrupted by **noise**, making the neural response stochastic. Decisions are intrinsically non-deterministic, and this probably has an advantage: the environment is not deterministic, so we would be worse if we were deterministic machines. 

We distinguish between **perceptual DM** and **value-based DM**. The first are about external signals. The latter are also called *economic decision making*, and are about the subjective preference or value. 

There are certain rules that a normal subject should follow, though: there is a certain consistency in how a subject decides. For instance, if you prefer A to B, and B to C, you should prefer A to C too. There is some transitivity in these rules, even though sometimes exceptions happen. 

We assume that the agent should maximize reward, so to avoid punishments etc... This is tricky: someone may have a taste for punishment, preferring to lose rather than winning. Thank god, machines don't have this possibility.

Economic decision making are made daily by us. Values and selection are strictly tied, when we select, we value things. When we are valuing choices, we are starting to select them. Then, finally, we arrive to the *consumation* phase, the one in which we take the action. 

Utility is difficult to measure: we can understand it after the choice taking, it's called *reveal utility*. It is a subjective measure of pleasure/satisfaction derived from the individual's preferences. 

You could also ask *how much are you willing to work for something*. This has a different meaning: it's a very operational definition of reward, *anything the animal is wanting to work for.* 

The liking and wanting are probably operating on two different mechanisms: think about drugs, nobody likes heroin, yet everybody likes heroin ;)

There are several interesting areas involved in this concept, like the amygdala, the insula, the cingulate... Food satiation is a huge topic: for example, we are concerned by **selective satiation**, meaning that even if we're full of pasta, we'll still eat the dessert. This is interesting: we are biased away from one food towards another one. This is just one example of the possible areas of valuation and the manipulation of value signals. There are many ways in which we can devalue an object, for example, someone gives us 30$ and shows us objects to buy with it without telling us the price. The price is very high: as soon as we see the price, our interest goes down. This is another example of how our valuation varies: everybody would like a BMW 1 series, but nobody would want to pay 40k for a hatchback!

## Reinforcement learning

RL is not just for computers: it happens in the brain too! We'd now like to discover the neural correlates of this. But before of that, we should study the psychology behind it: it's nice to state that the creator of RL was a psychologist. RL is about mapping states to actions, in order to optimize the amount of reward we get in the long run. There are different types of learning, this is *almost supervised*, though there's other supervised and unsupervised methods. It's interesting that computer scientists have been studying psychology. Doya associated 3 types of learning (unsupervised, supervised, reinforcement) to 3 different areas of the brain, the basal ganglia (RL), the cerebral cortex (UL) and the cerebellum (SL).

Behavioral psychology is divided into two many fields: learning to make prediction, and the *classical pavlovian conditioning*, instrumental conditioning, about action selection. Two major topics that are parallel in the prediction and action selection topics. In the Pavlovian learning (conditioning on learning), it's going to learn some link between stimulae, and it's not important whether the action is voluntary: the reward is still achieved. The reward depends on the stimulus, it's not dependent on the action. In the other case, the instrumental conditioning, the reward depends on the action: if the animal doesn't do aniything, it gets no reward. 

> *Instrumental: mean to reach the reward.*

Food is an unconditioned reward: it doesn't require any training. You present food to the animal, it salivates. If you pair the food with a bell (conditioning), first the bell then the food, the animal will start to salivate when it hears a bell!

### Higher order conditioning

After having conditioned the animal with the bell, we can transfer this. If we associate the bell with a light, the light will trigger the salivation too! Obviously, the response will be weaker. We could add a fourth stimula, and so on... 

The strength of learning (we measure by salivation) is very strong at the beginning, but if we keep tricking the animal, it will go down fast. Remember that some learning never disappears (latent learning), so if we re-present the animal the reward, the learning curve will go up even faster!

The RL roots go back to the Rescorla-Wagner model, which states that you learn events when they **violate expectations**.  
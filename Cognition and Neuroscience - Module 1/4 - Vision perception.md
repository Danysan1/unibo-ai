# 4 - Vision perception

How does the vision system percept immediate input? There's lots of information, and we'd like to extract a generalization. The fact that eyes have different positions influences the visual field: if you close your left eye you lose some of the sight. The representation overlaps, but not fully!

There's lag between the perception of a *moving object* by the two eyes. The representation of the object in the two eyes is changing. The model will lose specificity to achieve the perception of the movement: moving objects are more difficult to observe for us. This characteristic can allow us to start thinking at lower levels. If we move forward than this, and we pick movement as a feature we want to observe, among edges, motion, binocular disparity, color, these can be considered as low-level properties, the combination of which allow us to perform feature detection. 

The information that in front of us is a young lady is achieved further on. One of the reason this is is that we are able to spot little details. This is an important element to take into consideration. What is the role of early sensory areas like V1 in this?

There are areas which are coding for more abstract information. There's no simple explanation for these results. It's just one important aspect that needs further information. 

An influential way of understanding information processing in V1 is viewing these neurons as feature detectors: we already have sensations in response of the retina, and at the level V1 we have no perception of faces, for example, but all the elements that determine it: color, shape, motion, edges...

It's crucial to think that the system is heavily organized, we're no longer talking about anatomy. This is not a characteristic of the visual system alone: other areas are built like this.

When we record EG activity in the brain, what we see is alpha activity, which might serve this thalamo-cortico-thalamic inhibitory connections.

Continuous EEG recordings are online ot the TMS, and we're interested in knowing whether anything before the TMS pulse influenced the ability of the patient to succeed.

*So, do spontaneous fluctuations in posterior alpha-band EEG activity reflect variability in excitabiliy of human visual areas?*

The amplitutde of alpha is very important: if there's a reduction, we'll have a clear predisposition to excitability in the visual cortex, i.e. more likely to perceive information. If there's less, there's a sort of inhibition, we're less likely to perceive incoming information. The result is the subject might well use alpha activity information as a gate towards perceived informations, i.e. opening/closing a gate for incoming informations to regulate the quantity of perceived infos. 

Another concept to denote is that we have two service systems that are somehow converging, but we can dissociate them. What we think we're seeing, and what we actually see. We might believe in something that is not there. These two dimensions of alpha oscillations can be dissociated to providee evidence of these two different mechanisms. The previously presented model is the one in which we have the capacity to optimize our response depending on our capacity to represent the environment. Our model will therefore be dissociated with the actual thing. Imagine an AI system that has *forging what is going to happen next* as a goal, this knowledge shaping will become crucial.

If these two mechanisms do not work well together, we'll incur in problems!

As we cannot really tell which site is gonna be necessary when the stimulus will be presented, we have two possibilities: *low cortical excitability* (the subject is more *conservative*, i.e. *probably the stimulus ain't gonna be there*), or *high cortical excitability*.

Alpha speed accounts for the spatio-temporal resolution of the visual system and predicts objective accuracy. Faster alpha oscillations will lead to higher sampling resolution.

## Correlation is not causation

We'd like to draw a causal link between perception and sensation. 
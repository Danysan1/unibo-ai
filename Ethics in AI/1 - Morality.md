<!-- Slides 0_L2SartorEthicsIntro.pdf -->
# Morality

### Positive vs Critical morality
**<ins>Positive (conventional) morality</ins>**: the moral rules and principles that are accepted in a society. "Positive" in this context does not necessarily mean that it's good but that it's actually in use and applied in the real world

**<ins>Critical morality</ins>**: The morality that is correct, rational, just (maybe since considers all individual and social interests at stake giving each one the due significance). We can criticize positive morality based on our critical morality: we may be right or wrong.

Morality may coincide, be enforced by or be against law, religion, tradition and/or self-interest.

### Normative ethics vs Meta-ethics
**<ins>Normative ethics</ins>** is concerned with determining what is morally required, how one ought to behave

**<ins>Meta-ethics</ins>** is concerned with is the study of the nature, scope, and meaning of moral judgement
### Absolutism vs Relativism
**<ins>Absolutism</ins>**: There is a single true ethics: when two people express incompatible ethical judgement one of them must be wrong

**<ins>Relativism</ins>**: Ethical judgments are always relative to particular frameworks of attitudes

<!-- Prima facie duties are discussed later -->

-----

<!-- Slides 1_L2SartorEthicsConsequentilism.pdf -->
# <ins>Consequentialism</ins>

Thinks morality as an optimization problem to **get the global highest utility**. An action is morally required if and only if
* it delivers the best outcome, relative to its alternatives
* its good outcomes outweigh its negative outcomes to the largest extent
* it produces the highest utility

Various kinds (what is included in utility calculation and how?):
## <ins>Utilitarianism</ins>
**Mill’s principle of utility**: Actions are **right in proportion as they tend to promote happiness**, wrong as they tend to produce the reverse of happiness. By happiness is intended pleasure, and the absence of pain; by unhappiness, pain, and the privation of pleasure.
* Conceptually simple
* Egalitarian (everybody’s utility counts in the same way)
* Fits with some basic intuitions (making people happy is good, making them suffer is bad)
* In many case it is workable, in some cases problematic (what should we do about hunger, how shall we treat friends and relatives, etc.)

Utilitarianism favors (modest) redistribution of wealth, since the same amount of money gives more utility to the poor than to the rich. The impact of redistribution on wealth generation however has to be considered. Wealth maximization (adopted by some economic approach) aims at maximizing the wealth in society regardless of distribution, going in contrast with utilitarianism.
### <ins>Act utilitarianism</ins>
Do the **action that maximizes utility**.
* Can individuals, or even AI systems, accurately calculate and optimize the outcomes of their actions? The availability of information for such calculations may be limited.
* Should I give to the poor all that I have above the minimum that allows me to survive?
* Should I give the same importance to everybody, regardless of their connection to me?
* Is it OK to harm some people for the greater benefit of others? (Reprisals? Torture? Sadism?)
### <ins>Rule utilitarianism</ins>
Follow a **social rule the general compliance with which would provide the highest utility**.
* What about those exceptional cases in which the rule does not deliver?
* What if you know that most people are not following the rule? Should we be honest if most people around as are dishonest?
# Social dilemmas
## <ins>Trolley problem</ins>
* Standard: should you switch “the switch” to divert a train from a rail branch where it would kill many people to a branch where it would kill only one?
* Variant with no switch but ability to sacrifice someone (“the fat man”) pushing him on the rail to stop the train before hitting the others

## The social dilemma of <ins>autonomous vehicles</ins>
* Should a car divert and kill someone on a sidewalk to save many on the road?
    * What if people is illegally crossing with a red light? In utilitarianism this would be irrelevant as we only consider global utility 
* Should a car divert and kill its passenger to save someone on the road?
* Should a car divert and kill its passenger to save many on the road?
  * In utilitarianism yes, but a human driver would probably not do that
  * Idea of an "**ethical knob**" to adjust the behavior of these vehicles in morally challenging situations based on their own ethical preferences. This is a controversial as it could lead to a lack of uniformity and predictability in how vehicles behave in similar situations. Many experts argue that decisions regarding ethical trade-offs in autonomous vehicles should be based on societal consensus rather than individual preference.

## The surgeon case by Judith Jarvis Thomson
Each of five patients are in need of a different organ, each of whom will die without that organ, but none is available. A traveler passes by the hospital and its disappearance would not be noted by anyone. Should the surgeon kill him to gather organs and save other patients?
* Act utilitarianism would say yes, but the morality of this is questionable
* In rule utilitarianism this would be partially solved as acting like this would make people stay away from hospitals and make life harder for everybody.

-----

<!-- Slides 2_SartorEthicsIntroDeontology.pdf -->
# <ins>Deontology</ins>

Certain actions are good or bad regardless of their consequences: what makes a choice right is its **conformity with a moral norm** which order or permits it, rather than its good of bad effect.

Issue: Is it considered unethical to have preferences for oneself or one’s friends? The **<ins>Golden Rule</ins>** is often invoked as a guideline for ethical behavior (however its complex to apply it universally):
* **Treat others as you would like others to treat you**
* **Do not treat others in ways that you would not like to be treated**
* **What you wish upon others, you wish upon yourself**

## Kantian ethics
**<ins>Hypothetical imperatives</ins>**: Moral guidelines that require us to do **what fits our personal goals** or desires.

**<ins>Categorical imperative</ins>**: moral guideline that moral principle that **applies universally to all rational beings**, regardless of personal wants and desires.

**Kant**’s first formulation of the categorical imperative is the **<ins>principle of universalizability</ins>**: "**Act only according to that maxim by which you can at the same time will that it should become a universal law**".
A maxim is a subjective principle of action, an intention to perform an action for a certain reason.
**Landau’s test of universalizability**:
1. Formulate your maxim clearly state what you intend to do, and why you intend to do it.
2. Imagine a world in which everyone supports and acts on your maxim.
3. Then ask: Can the goal of my action be achieved in such a world?

An alternative formulation of the categorical imperative is the **<ins>principle of humanity</ins>**: "so act that you treat humanity in your own person and in the person of everyone else always at the same time as an end and never merely as means". Rational beings capable of morality have a **dignity** and should **never be treated as mere means to an end**, without considering their values and purposes.

For Kant if we follow rationality, we have to be moral:
1. If you are rational, then you are consistent.
2. If you are consistent, then you obey the principle of universalizability.
3. If you obey the principle of universalizability, then you act morally.
4. Therefore, if you are rational, then you act morally.
5. Therefore, if you act immorally, then you are irrational.

A robot based on Kantian ethics would be consistent and impartial but may cat on bad or too rigid maxims.

## Prima facie duties

**<ins>Pro-tanto moral judgment</ins>**: Many moral prescription are defeasible. They state general propositions that are susceptible of exceptions. For example: we should no lie, but what if a lie would save a person’s life?

Ross' **<ins>Prima facie duties</ins>**: rules we should follow because there is a moral reason in favor of doing the act, but one that can be outweighed by other (moral) reasons:
* **Fidelity**: We should strive to keep promises and be honest and truthful.
* **Reparation**: We should make amends when we have wronged someone else.
* **Gratitude**: We should be grateful to others when they perform actions that benefit us and we should try to return the favour.
* **Non injury (or non maleficence)**: We should refrain from harming others either physically or psychologically.
* **Beneficence**: We should be kind to others and to try to improve their health, wisdom, security, happiness, and well being.
* **Self improvement**: We should strive to improve our own health, wisdom, security, happiness, and well being.
* **Justice**: We should try to be fair and try to distribute benefits and burdens equably and evenly.

# Contractarianism

### <ins>Social contract theories</ins>

**In political theory** a societal arrangement is just if it has been (or would have been) accepted by free and rational people.

**In moral theory (Shafer-Landau)** actions are morally right just because they are permitted by rules that free, equal, and rational people would agree to live by, on the condition that others obey these rules as well.

**Rawls' theory of justice**: to ensue that social contracts are fair people should choose them under a "**veil of ignorance**" as if they didn't know their gender, social position interests talents, wealth, race, etc. Two principles should be applied:
1. (with priority) Each person has an equal indefeasible claim to a fully adequate scheme of equal basic liberties, compatible with the same liberties for all. This includes freedom of conscience, freedom of association, freedom of speech, liberty of the person, and the right to vote, ...
2. Social and economic inequalities are permissible only if they meet two conditions. First, they must be attached to offices and positions that are open to all under fair equality of opportunity. Second, they must work to the greatest benefit of the least-advantaged members of society, known as the difference principle.

**Habermas' Discourse Ethics**: A **rule of action or choice is justified**, and thus valid, **only if all those affected by the rule or choice could accept it in a reasonable discourse**.
A norm is valid when the foreseeable consequences and side effects of its general observance for the interests and value orientations of each individual could be jointly accepted by all concerned without coercion.

# Virtue ethics

Ethics should not focus on norms nor on consequences. An act is morally right just because it is one that a virtuous person. The right act is that that would result from the mix of the relevant virtues: honesty, loyalty, courage, impartiality, wisdom, fidelity, generosity, compassion, etc.

-----

<!-- Slides 3_GS2012SlidesGameTheoryLaw2.pdf -->
# <ins>Game theory</ins>

**Rationality as maximization of preference-satisfaction**: given a set of alternatives $X = x_1, x_2, \dots, x_n$ the agent has a preference $x \succsim y$ ("x is weakly preferred to y").

$x_i \succ x_j \iff x_i \succsim x_j \land x_j \nsucceq x_i$

$x_i \approx x_j \iff x_i \succsim x_j \land \space x_j \succeq x_i$

Multiple  definitions of **rationality** exist, it's often described as:
* choosing the most preferred option (the option that has a highest utility)
* the choice reveals consistent preferences:
  * **Reflexivity**: $\forall x_i \space x_i \succsim x_i$ (every alternative is valuable as much as itself)
  * **Completeness**: $\forall x_i \space \forall x_j \space x_i \succsim x_j \lor x_j \succsim x_i$ (all alternatives are comparable: either xi is at least as valuable as xj or xj is at least as valuable as xi)
  * **Transitivity**: $\forall x_i \space \forall x_j \space x_i \succsim x_j \land x_j \succsim x_k \rArr x_i \succsim x_k$

An **<ins>utility function</ins>** $u: X \rarr \mathbb{R}$ associates to each alternativa a number such that $\forall x_i \space \forall x_j \space x_i \succsim x_j \iff u(x_i) \ge u(x_j)$ and $x_i \succ x_j \iff u(x_i) \gt u(x_j)$. Ordinal utilities reflect only the order of preference, cardinal utilities reflect also the importance of the preference.

**Expected utility** of an action: utility of each alternative result of the action, multiplied for the probability of that consequence.

Given a set of players $N = p_1, p_2, \dots, p_n$ and for each player a set of available actions $A_{p_i}$ the set of action profiles A contains the combinations of actions of the players. Each player has a payoff function that associates each action profile to a number. With two player these can be represented on a table, for example:

$N = \{Row,Col\}$

$A_{Row} = A_{Col} = \{flic,floc\}$

$A = \{(flic_{Row},flic_{Col}), \space (flic_{Row},floc_{Col}), \space (floc_{Row},flic_{Col}), \space (floc_{Row},floc_{Col})\}$

$u_{Row}(flic_{Row},flic_{Col})=1 , \space u_{Row}(flic_{Row},floc_{Col})=0, \space u_{Row}(floc_{Row},flic_{Col})=0 , \space u_{Row}(floc_{Row},floc_{Col})=1$

R \ C | flic | floc
------|------|------
flic  | 1, 1 | 0, 0
floc  | 0, 0 | 1, 1

### Prisoner dilemma

Scenario that involves two individuals who have been arrested and are being held in separate cells.
* if both betray each other and cooperate with the prosecution, they each receive a moderate punishment
* if both remain silent and cooperate with each other, they both receive a lighter sentence
* if one betrays while the other remains silent, the betrayer goes free while the silent one receives a harsh punishment

R \ C           | Confess | $\lnot$ Confess
----------------|---------|--------------
Confess         | -3, -3  | 0, -4
$\lnot$ Confess | -4, 0   | -1, -1


It illustrates the conflict between cooperation and self-interest between countries, businesses, traffic, ... .

### Nash equilibrium

A strategy profile $(a_1, a_2, \dots, a_n)$ where each $a_i$ is the strategy of player $p_i$, represents a Nash equilibrium, if no player could improve its payoff by changing its strategy alone (when the other players continue to play the actions in the profile). This implies that each player’s action in the profile is the best response to the other actions in the profile.

### Extensive form games

A game is specified by indicating
* the players
* when each player can act
* what choices are available to each player when that player can act
* what each player knows about the other players moves
* the payoffs for each combination (sequence) of actions

<!--```mermaid
flowchart TD;
    A ->|Lender does not loan| B[(0,0)]
    A ->|Lender loans| C
    C ->|Debtor defaults| D[(-100,110)];
    C ->|Debtor pays| E[(5,5)];
```-->

# Ethics in Artificial Intelligence

<!-- Slides 1 -->

### Positive vs Critical morality
**<u>Positive (conventional) morality</u>**: the moral rules and principles that are accepted in a society. "Positive" in this context does not necessarily mean that it's good but that it's actually in use and applied in the real world

**<u>Critical morality</u>**: The morality that is correct, rational, just (maybe since considers all individual and social interests at stake giving each one the due significance). We can criticize positive morality based on our critical morality: we may be right or wrong.
### Normative ethics vs Meta-ethics
**<u>Normative ethics</u>** is concerned with determining what is morally required, how one ought to behave

**<u>Meta-ethics</u>** is concerned with is the study of the nature, scope, and meaning of moral judgement
### Absolutism vs Relativism
**<u>Absolutism</u>**: There is a single true ethics: when two people express incompatible ethical judgement one of them must be wrong

**<u>Relativism</u>**: Ethical judgments are always relative to particular frameworks of attitudes

<!-- Prima facie duties are discussed later -->

### Morality and other normative systems
Morality may coincide, be enforced by or be against law, religion, tradition and/or self-interest.

-----
<!-- Slides 2 -->

# <u>Consequentialism</u>
Thinks morality as an optimization problem to get the global highest utility. An action is morally required if and only if
* it delivers that best outcome, relative to its alternative
* its good outcomes outweigh its negative outcomes to the largest extent
* it produces the highest utility

Various kinds (what is included in utility calculation and how?):
## <u>Utilitarianism</u>
Mill’s principle of utility: Actions are right in proportion as they tend to promote happiness, wrong as they tend to produce the reverse of happiness. By happiness is intended pleasure, and the absence of pain; by unhappiness, pain, and the privation of pleasure.
* Conceptually simple
* Egalitarian (everybody’s utility counts in the same way)
* Fits with some basic intuitions (making people happy is good, making them suffer is bad)
* In many case it is workable, in some cases problematic (what should we do about hunger, how shall we treat friends and relatives, etc.)

Utilitarianism favors (modest) redistribution of wealth, since the same amount of money gives more utility to the poor than to the rich. The impact of redistribution on wealth generation however has to be considered. Wealth maximization (adopted by some economic approach) aims at maximizing the wealth in society regardless of distribution, going in contrast with utilitarianism.
### <u>Act utilitarianism</u>
Do the action that maximizes utility. 
* Should I give to the poor all that I above the minimum that allows me to survive?
* Should I give the same importance to everybody, regardless of their connection to me?
* Is it OK to harm some people for the greater benefit of others? (Reprisals? Torture? Sadism?)
### <u>Rule utilitarianism</u>
Follow a social rule the general compliance with which would provide the highest utility.
* What about those exceptional cases in which the rule does not deliver?
* What if you know that most people are not following the rule? Should we be honest if most people around as are dishonest?
# Social dilemmas
## <u>Trolley problem</u>
* Standard: should you switch “the switch” to divert a train from a rail branch where it would kill many people to a branch where it would kill only one?
* Variant with no switch but ability to sacrifice someone (“the fat man”) pushing him on the rail to stop the train before hitting the others

## The social dilemma of autonomous vehicles
* Should a car divert and kill someone on a sidewalk to save many on the road?
    * What if people is illegally crossing with a red light? In utilitarianism this would be irrelevant as we only consider global utility 
* Should a car divert and kill its passenger to save someone on the road?
* Should a car divert and kill its passenger to save many on the road?
* In utilitarianism yes, but a human driver would probably not do that

## The surgeon case by Judith Jarvis Thomson
Each of five patients are in need of a different organ, each of whom will die without that organ, but none is available. A traveler passes by the hospital and its disappearance would not be noted by anyone. Should the surgeon kill him to gather organs and save other patients?
* Act utilitarianism would say yes, but the morality of this is questionable
* In rule utilitarianism this would be partially solved as acting like this would make people stay away from hospitals and make life harder for everybody.

-----
<!-- Slides 3 -->

# <u>Deontology</u>
Certain actions are good or bad regardless of their consequences: what makes a choice right is its conformity with a moral norm which order or permits it, rather than its good of bad effect.

Is it considered unethical to have preferences for oneself or one’s friends? The Golden Rule is often invoked as a guideline for ethical behavior (however its complex to apply it universally):
    • Treat others as you would like others to treat you
    • Do not treat others in ways that you would not like to be treated
    • What you wish upon others, you wish upon yourself
## Kantian ethics
**<u>Hypothetical imperatives</u>**: Moral guidelines that require us to do what fits our personal goals or desires.

**<u>Categorical imperative</u>**: moral guideline that moral principle that applies universally to all rational beings, regardless of personal wants and desires.

Kant’s first formulation of the categorical imperative is the **<u>principle of universalizability</u>**: “Act only according to that maxim by which you can at the same time will that it should become a universal law”. A maxim is a subjective principle of action, an intention to perform an action for a certain reason.
Landau’s test of universalizability:
1. Formulate your maxim clearly state what you intend to do, and why you intend to do it.
2. Imagine a world in which everyone supports and acts on your maxim.
3. Then ask: Can the goal of my action be achieved in such a world?

An alternative formulation of the categorical imperative is the **<u>principle of humanity</u>**: “so act that you treat humanity in your own person and in the person of everyone else always at the same time as an end and never merely as means”. This means that we and our tools should never treat people only as means, without considering their values and purposes. Rational beings capable of morality have a dignity and should not be treated as mere means to an end.

For Kant if we follow rationality, we have to be moral:
1. If you are rational, then you are consistent.
2. If you are consistent, then you obey the principle of universalizability.
3. If you obey the principle of universalizability, then you act morally.
4. Therefore, if you are rational, then you act morally.
5. Therefore, if you act immorally, then you are irrational.

A robot based on Kantian ethics would be consistent and impartial but may cat on bad or too rigid maxims.

## Prima facie duties

**<u>Pro-tanto moral judgment</u>**: Many moral prescription are defeasible. They state general propositions that are susceptible of exceptions. For example: we should no lie, but what if a lie would save a person’s life?

Ross' **<u>Prima facie duties</u>**: rules we should follow because there is a moral reason in favor of doing the act, but one that can be outweighed by other (moral) reasons:
* **Fidelity**: We should strive to keep promises and be honest and truthful.
* **Reparation**: We should make amends when we have wronged someone else.
* **Gratitude**: We should be grateful to others when they perform actions that
benefit us and we should try to return the favour.
* **Non injury (or non maleficence)**: We should refrain from harming others either
physically or psychologically.
* **Beneficence**: We should be kind to others and to try to improve their health,
wisdom, security, happiness, and well being.
* **Self improvement**: We should strive to improve our own health, wisdom, security,
happiness, and well being.
* **Justice**: We should try to be fair and try to distribute benefits and burdens
equably and evenly.

<!-- TODO -->

-----
<!-- Slides 4 -->

<!-- TODO -->
\documentclass[]{article}
\usepackage[a4paper, portrait, margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\setlength\parindent{0pt}

\newcommand{\cfi}[1]{f_{x_{#1}}(\vec{x})}
\newcommand{\pfi}[1]{\frac{\partial f}{\partial x_{#1}}(\vec{x})}

\newcommand{\scfi}[2]{f_{x_{#1}x_{#2}}(\vec{x})}
\newcommand{\spfi}[2]{\frac{\partial^2 f}{\partial x_{#1} \partial x_{#2}}(\vec{x})}

%opening
\title{Formulary \\ \large Statistical and Mathematical Methods for Artificial Intelligence}
\author{Daniele Santini}

\begin{document}
	
	%\maketitle
	
	\section{Numerical computation and finite numbers}
	
	Absolute error $E_x = \tilde{x} - x$
	
	Relative error $R_x = \dfrac{\tilde{x} - x}{x}$
	
	$x \in \mathbb{R}, \ fl(x) \in \mathcal{F}(\beta,t,L,U) \Leftrightarrow fl(x) = \pm (d_1 \beta^-1 + d_2 \beta^-2 + \dots + d_t \beta^-t) \beta^p = \pm m \beta^p$ with $0 \le d_i \le \beta-1$, $\ L \le p \le U$ ($\beta$ "base", $t$ "precision", $m$ "mantissa", $p$ "exponent").
	
	$UFL = \beta^{L-1}$;
	$\ OFL = \beta^U (1-\beta^{-t})$
	
	Machine precision with rounding by chopping: $\epsilon_{mach} = \beta^{1-t}$; rounding to nearest: $\epsilon_{mach} = \frac{1}{2}\beta^{1-t}$.
	
	\section{Eigenvectors and eigenvalues}
	
	$A \in \mathbb{R}^{nxn}; A \vec{x} = \lambda \vec{x} \Leftrightarrow \vec{x}$ eigenvector and $\lambda$ eigenvalue of $A$.
	
	$A$ triangular or symmetric $\Rightarrow$ eigenvalues are on the main diagonal.
	
	Spectrum $\sigma(A) = \{\vec{x}: \vec{x}$ eigenvector of $A\}$.
	Spectral norm $\rho(A) = \max \lvert \lambda \rvert$
	
	$C \in \mathbb{R}^{nxn}$ singular $\Leftrightarrow \det(C)=0$
	
	Similarity transformation: $A, C \in \mathbb{R}^{nxn}; \ C$ non-singular; $A$ and $C^{-1} A C$ are similar (same spectrum and eigenvalues).
	
	
	
	%TODO
	
	
	
	$A \in \mathbb{R}^{mxn} \Rightarrow A^T A \in \mathbb{R}^{nxn}$ is positive semi-definite.
	
	$A \in \mathbb{R}^{mxn}$ with maximum rank $(rk(A)=\min(m,n)) \Rightarrow A^T A \in \mathbb{R}^{nxn}$ is positive definite.
	
	Spectral theorem: $A \in \mathbb{R}^{nxn}$ symmetric $\Rightarrow$ eigenvalues are real, eigenvectors create an orthogonal basis.
	
	\section{Norm}
	
	Scalar product: $\vec{x},\vec{y} \in V=\mathbb{R}^n, \ \langle \vec{x},\vec{y} \rangle = \sum\limits_{i=1}^{n} x_i y_i$
	
	$\lVert \vec{x} \rVert \ge 0 \ \forall \vec{x} \in V$;
	$\lVert \vec{x} \rVert=0 \iff \vec{x}=\vec{0}$;	
	$\lVert \alpha \vec{x} \rVert = \lvert \alpha \rvert \lVert \vec{x} \rVert \ \forall \alpha \in \mathbb{R}, \vec{x} \in V$;	
	$\lVert \vec{x}+\vec{y} \rVert \le \lVert \vec{x} \rVert + \lVert \vec{y} \rVert \ \forall \vec{x},\vec{y} \in V$.
	
	p-norm: $p\in [1,\infty[, \ \lVert \vec{x} \rVert_p = \sqrt[p]{\sum_{i=1}^n \lvert x_i \rvert^p}$
	\begin{itemize}
		\item 1-norm (a.k.a. Manhattan norm): $\lVert \vec{x} \rVert_1 = \sum\limits_{i=1}^n \lvert x_i \rvert$
		\item 2-norm (a.k.a. Euclidean norm): $\lVert \vec{x} \rVert_2 = \sqrt{\sum\limits_{i=1}^n \lvert x_i \rvert^2} = \sqrt{\langle \vec{x},\vec{x} \rangle}$
		\item Infinity-norm: $\lVert \vec{x} \rVert_\infty = \max{\lvert x_i \rvert}$
	\end{itemize}

	Distance $d(\vec{x},\vec{y}) = \lVert \vec{y}-\vec{x} \rVert$

	\subsection{Matrix norm}
	
	Similar properties of vector norm, plus $\lVert A B \rVert \le \lVert A \rVert \lVert B \rVert \ \forall A,B\in \mathbb{R}^{nxn}$
	 
	Frobinius norm: $\lVert A \rVert_f = \sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2}$
	
	p-induced matrix norm: 
	\begin{itemize}
		\item 1-norm: $\lVert A \rVert_1 = \max\limits_{j=1..n}\sum\limits_{i=1}^m \lvert a_{ij} \rvert$
		\item 2-norm: $\lVert A \rVert_2 = \sqrt{\rho(A^T A)}$
		\item Infinity-norm: $\lVert A \rVert_\infty = \lVert A^T \rVert_1 = \max\limits_{i=1..m}\sum\limits_{j=1}^n \lvert a_{ij} \rvert$
	\end{itemize}

	Condition number $K_p(A) = \lVert A \rVert_p \lVert A^{-1} \rVert_p$.  $\ A$ singular $\Leftrightarrow K_p(A) = \infty$. $\ K_2(A) = \dfrac{\lambda_{max}}{\lambda_{min}}$
	
	%\section{Projections} %TODO
	
	%TODO
	
	\section{Matrix decompositions / factorizations}
	
	\subsection{LU decomposition}
	
	$A \in \mathbb{R}^{nxn} $ non-singular ($det(A)\ne0$) with all principal minors non-singular $\Rightarrow A = L U$ with $L \in \mathbb{R}^{nxn}$ lower triangular and $U \in \mathbb{R}^{nxn}$ upper triangular.
	
	\subsection{Cholesky factorization}
	
	$A \in \mathbb{R}^{nxn} $ positive definite $\Rightarrow A = L L^T$ with $L \in \mathbb{R}^{nxn}$ lower triangular.
	
	\subsection{Singular Value Decomposition (SVD)}
	
	$A \in \mathbb{R}^{mxn}, r = rk(A)\in[0,min(m,n)] \Rightarrow A = U \Sigma V^T$ with 
	\begin{itemize}
		\item $U\in\mathbb{R}^{mxm}$ orthogonal.
		\item $V\in\mathbb{R}^{nxn}$ orthogonal.
		\item $\Sigma \in\mathbb{R}^{mxn}$ with $\Sigma_{ii}=\sigma_i$ ("singular value") and $i \ne f \Rightarrow \Sigma_ij=0$.
	\end{itemize}
	$\sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_r > \sigma_{r+1} = \sigma_{r+2} = \ldots = \sigma_n = 0$.

	$\sigma_i = \sqrt{\lambda_i(A^T A)}$ where $\lambda_i(A)$ is the i-th eigenvalue of $A$ by value.
	
	\ $\sigma_1 = \sqrt{\rho(A^T A)} = \lVert A \rVert_2$.
	\ $\lVert A^{-1} \rVert_2 = \dfrac{1}{\sigma_r}$.
	\ $K_2(A) = \dfrac{\sigma_1}{\sigma_r}$.
	
	\subsubsection{Rank-k-approximation}
	$A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_i u_i v_i^T = \sum_{i=1}^r \sigma_i A_i$ with $u_i \in \mathbb{R}^m$ column of $U$ and $v_i \in \mathbb{R}^n$ column of $V$.
	$\hat{A}_k = \sum_{i=1}^k \sigma_i u_i v_i^T = \sum_{i=1}^k \sigma_i A_i$ with $k < r$ is the rank-k-approximation of $A$.
	
	\section{Vector calculus}
	
	Chain rule: $g(f(x))' = (g \circ f)'(x) = g'(f(x)) f'(x)$
	
	$f : \mathbb{R}^n \to \mathbb{R}$; 
	Partial derivative $\pfi{i}=\cfi{i}=\lim_{h\to 0}{\frac{f(x_{1},...,x_{i}+h,...,x_{n})-f(\vec{x})}{h}}$
	
	Gradient $\nabla f(\vec{x}) = \left(\pfi{1},...,\pfi{n}\right)$

	Second order partial derivative $\spfi{i}{j}=\scfi{i}{j}=\frac{\partial}{\partial x_{j}}\pfi{i}$
	
	Hessian $\nabla^2 f(\vec{x}) = H_{f}(\vec{x})=\left(\spfi{i}{j}\right)_{i,j=1,...,n} = \begin{pmatrix}
		\scfi{1}{1} & \cdots & \scfi{n}{1} \\
		\vdots & \ddots & \vdots \\
		\scfi{1}{n} & \cdots & \scfi{n}{n}
	\end{pmatrix}$

	\subsection{Useful identities for computing gradients}
	
	$\vec{x}, \vec{a}, \vec{b} \in \mathbb{R}^{n}$;
	$X \in \mathbb{R}^{nxn}$;
	
	$\frac{\partial \vec{f}(X)^T}{\partial X} = \left(\frac{\partial \vec{f}(X)}{\partial X}\right)^T$;
	
	$\frac{\partial \vec{f}(X)^{-1}}{\partial X}  = - \vec{f}(X)^{-1} \frac{\partial \vec{f}(X)}{\partial X} \vec{f}(X)^{-1}$
	
	$\frac{\partial \vec{x}^T \vec{a}}{\partial \vec{x}} = \frac{\partial \vec{a}^T \vec{x}}{\partial \vec{x}} = \vec{a}^T$;
	
	$\frac{\partial}{\partial X} \vec{a}^T X \vec{b} \vec{a} = \vec{a}^T \vec{b}$;
	
	$\frac{\partial \vec(a)^T X \vec{a}}{\partial \vec{a}} = \vec{a}^T (X + X^T)$;
	
	$\frac{\partial \lVert a-Xb\rVert_2^2}{\partial X} = \frac{\partial \lVert Xb-a\rVert_2^2}{\partial X} = 2(Xb-a)^T X = 2 X^T (Xb-a) = 2(X^T X b - X^T a)$;

	\section{Linear systems}
	
	$A\vec{x} = \vec{b}$ with $A \in \mathbb{R}^{mxn}, \vec{x} \in \mathbb{R}^n, \vec{b} \in \mathbb{R}^m$.
	$m=n \Leftrightarrow$ Square linear system.
	
	$A^{-1}$ exists $\iff \vec{x} = A^{-1} \vec{b}$
	
	\subsection{Least squares problem}
	
	$\vec{x}^*$ solution of $A \vec{x} = \vec{b} \ \Rightarrow \ \vec{x}^* \approxeq \arg\min\limits_{\vec{x}\in \mathbb{R}}\lVert A \vec{x} - \vec{b} \rVert^2$ (strictly convex, only one minimum which is global).
	
	\section{Optimization}
	
	$\max\limits_{\vec{x}\in\mathbb{R}} f(\vec{x}) = -\min\limits_{\vec{x}\in\mathbb{R}} -f(\vec{x})$; \ 
	$\arg\max\limits_{\vec{x}\in\mathbb{R}} f(\vec{x}) = \arg\min\limits_{\vec{x}\in\mathbb{R}} -f(\vec{x})$; \ 
	We search $\arg\min\limits_{\vec{x}\in\mathbb{R}} f(\vec{x})$
	
	\subsection{Iterative methods}
	
	$\alpha_k \in \mathbb{R}$ step length; $\vec{p}_k \in \mathbb{R}^n$ descent direction for $f$ in $\vec{x}_k$ ($\vec{p}_k^T \cdot \nabla f(\vec{x}_k) < 0$).
	
	$while ( k < kMax \ \land \ \lVert \nabla f(\vec{x}_k) \rVert < tolf \ \land \ \lVert \vec{x}_k - \vec{x}_{k-1} \rVert \ge tolx ) \  \vec{x}_{k+1} = \vec{x}_k + \alpha_k \vec{p}_k$
	
	Convergence speed:
	\begin{itemize}
		\item Q-linear: $\exists r\in]0,1[, \vec{x}^*, k*: \  \lVert \vec{x}_{k+1}-\vec{x}^* \rVert \le r \lVert x_k \rVert \ \forall k > k^*$
		\item Q-quadratic $\exists M>0, \vec{x}^*, k*: \  \lVert \vec{x}_{k+1}-\vec{x}^* \rVert \le M \lVert x_k \rVert^2 \ \forall k > k^*$
	\end{itemize}
	
	\subsubsection{Gradient Descent method}
		
	Q-linear, uses only first order gradient: $\vec{x}_{k+1} = \vec{x}_k - \alpha_k \nabla f(\vec{x}_k)$
	
	\subsubsection{Gradient Descent with momentum}
	
	$\vec{x}_{k+1} = \vec{x}_k - \alpha_k \nabla f(\vec{x}_k) + \beta_k (\vec{x}_i - \vec{x}_{i-1})$
	
	\subsubsection{Stochastic Gradient Descent}
	
	$L(\theta)=\sum\limits_{n=1}^N L_n(\theta); \ \vec{x}_{k+1} = \vec{x}_k - \alpha_k \nabla L(\theta)$ with:
	\begin{itemize}
		\item Ordinary Gradient Descent: $\nabla L(\theta) = \sum\limits_{n=1}^N \nabla L(\theta)$ 
		\item Random item: $\forall k \ i_k\in\{0,1,\dots,N\}; \ \nabla L(\theta) \approxeq \nabla L_{i_k}(\theta)$
		\item Mini-batch: $p < n; \ \forall k \ i_{1k},i_{2k},\dots,i_{pk}\in\{0,1,\dots,N\}; \ \nabla L(\theta) \approxeq \sum\limits_{j=1}^p \nabla L_{i_jk}(\theta)$
	\end{itemize}
	 
	
	\subsubsection{Newton method}
	
	Q-quadratic, uses also higher order info:
	$H_f(\vec{x}_k)\vec{p}_k = -\nabla^T f(\vec{x}_k)$ (linear system with solution $\vec{p}_k$)
	
	
	\section{Statistics}
	
	$\Omega$ sample space, $A \subseteq \Omega$ event space, $P:A\to[0,1]$ probability, $P(\Omega)=1$.
	
	\subsection{Discrete random variables}
	
	$X:A\to T\subset\mathbb{R}$ discrete random variable (Target/Support space $T$ finite or numerable); $x\in T$.
	
	Probability Mass Function $f_X(x) = P(X=x)$.
	$\sum\limits_{x \in T} f_X(x) = P(T) = 1$.
	
	Mean PMF $\mu=E(f_X)=\sum\limits_{x \in T}x f_X(x)$.
	Variance $\sigma^2 = \sum\limits_{x \in T} (x-\mu)^2 f_X(x)$.
	Standard deviation $\sigma=\sqrt{\sigma^2}$.
	
	Uniform distribution: $f_X(x)=\dfrac{1}{N}$.
	
	Poisson dist.: $f_X(x|\lambda)=e^{-\lambda}\dfrac{\lambda^x}{x!}$ ($\lambda$ mean of events in unit).
	$\mu=\lambda$. $\sigma=\lambda$.
	
	\subsection{Continuous random variables}
	
	$X:A\to T\subseteq\mathbb{R}$ continuous random variable; $x\in T$.
	
	$f_X:T\to\mathbb{R}$ Probability Density Function.
	$P(a\le x\le b) = \int_{a}^{b}f_X(x)dx$.
	$\int\limits_T f_X(x) dx = P(T) = 1$
	
	Mean PDF $\mu = E(f_X) = \int\limits_T x f_X(x) dx$.
	Variance $\sigma^2 = \int\limits_T (x-\mu)^2 f_X(x)dx$.
	Standard deviation $\sigma=\sqrt{\sigma^2}$.
	
	Gaussian/Normal distribution: $f_X(x|\mu,\sigma^2)=\dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)$
	
	Cumulative Distribution Function (for both discrete and continuous) $F_X:T\to[0,1]: F_X(x) = P(X\le x)$
	
	\subsection{Multivariate probability}
	
	$X:A\to T_X; \ Y:A\to T_Y; \ T_{XY}=T_X \times T_Y$.
	
	Joint probability $P(X=x,Y=y) = P(X=x \land Y=y)$.
	
	Marginal probability $P(X=x)=\left\{\begin{matrix}
		\sum_{y\in T_Y} P(X=x,Y=y) \text{ if Y discrete}\\
		\int_{T_Y} P(X=x,Y=y) dy \text{ if Y continuous}
	\end{matrix}\right.$.
	
	Conditional probability $P(\text { Effect } \mid \text { Cause })=\dfrac{P(\text { Effect } \wedge \text { Cause })}{P(\text { Cause })}$
	
	Bayes theorem: $P(\text { Cause } | \text { Effect })=\dfrac{P(\text { Effect } | \text { Cause }) P( \text{ Cause })}{P(\text { Effect })}$
	
	\subsection{Statistical and conditional independence}
	
	$Cov(x,y)=0 \ \Leftrightarrow \ P \models (A \perp B) \ \Leftrightarrow \ P(A \mid B)=P(A) \ \Leftrightarrow \ P(B \mid A)=P(B) \ \Leftrightarrow \ P(A,B)=P(A)P(B)$
	
	$P \models (A \perp B \mid C) \Leftrightarrow P(A|B,C) = P(A,C) \Leftrightarrow P(B|A,C) = P(B,C) \Leftrightarrow P(A,B|C) = P(A|C)P(B|C)$
	
	\section{Learning}
	
	$N$ observations $\vec{x}_n\in\mathbb{R}^D$ and labels $y_n\in\mathbb{R}$; $X=[\vec{x}_1,\dots,\vec{x}_n]^T$; $\vec{y}=[y_1,\dots,y_n]^T$; parameters $\vec{\theta}\in\mathbb{R}^{D}$
	
	\subsection{Empirical Risk Minimization}
	
	Linear model $f(\cdot,\vec{\theta}):\mathbb{R}^D\to\mathbb{R}:f(\vec{x})=\vec{\theta}^T \vec{x} + \theta_0 = \theta_0 + \sum\limits_{d=1}^D \theta_d x_{n,d}$
	
	We search $\vec{\theta}^* : \ f(\vec{x}_n, \vec{\theta}^*) = \hat{y}_n \approx y_n \ \forall n=1,2,\dots,N$
	
	Loss function $l(y, \hat{y})$; Empirical risk $R_{emp}(f,X,\vec{y},\vec{\theta})=\dfrac{1}{N}\sum\limits_{n=1}^N l(y_n,f(\vec{x}_n,\vec{\theta}))$
	
	$\vec{\theta}^* = \min\limits_{\vec{\theta} \in \mathbb{R}^D} R_{emp}(f,X,\vec{y},\vec{\theta}) = \min\limits_{\vec{\theta} \in \mathbb{R}^D} \dfrac{1}{N} \sum\limits_{n=1}^N (y_n - \vec{x}_n^T\vec{\theta})^2 = \min\limits_{\vec{\theta} \in \mathbb{R}^D} \dfrac{1}{N} \lVert \vec{y} - X \vec{\theta} \rVert^2$
	
	\subsection{Maximum Likelihood Estimation (ML)}
	
	Family of probability densities $p(\vec{x}\mid\vec{\theta})$;
	Loss $ \mathcal{L}_x(\vec{\theta}) = - \log p(\vec{x}\mid\vec{\theta})$;
	$\ \theta^* = \min_{\vec{\theta}} \mathcal{L}_x (\vec{\theta})$
	
	%$p(y_n\mid\vec{x},\vec{\theta}) \sim \mathcal{N}(y_n\mid\vec{x}_n^T \theta, \sigma_2)$.
	$p(\vec{y}\mid X,\vec{\theta})=\prod\limits_{n=1}^N p(y_n\mid\vec{x}_n,\vec{\theta}) \Rightarrow \mathcal{L}_x(\vec{\theta}) = - \log \left(\prod\limits_{n=1}^N p(y_n\mid\vec{x}_n,\vec{\theta})\right) = -\sum\limits_{n=1}^N \log p(y_n\mid\vec{x}_n,\vec{\theta})$
	
	$p(y_n\mid\vec{x}_n,\vec{\theta}) \sim \mathcal{N}(y_n - \vec{\theta}^T\vec{x}, \sigma^2) \Rightarrow \vec{\theta}^* = \min\limits_{\vec{\theta}} \dfrac{1}{2\sigma^2} \lVert\vec{y}-X\vec{\theta}\rVert_2^2$
	
	\subsection{Maximum A Posteriori Estimation (MAP)}
	
	$p(\vec{\theta}\mid\vec{x}) = \dfrac{p(\vec{x}\mid\vec{\theta}) p(\vec{\theta})}{p(\vec{x})}$;	
	$\ \vec{\theta}^* = \min\limits_{\vec{\theta}} -\log(p(\vec{\theta}\mid\vec{x})) =  \min\limits_{\vec{\theta}} - (\log (p(\vec{x}\mid\theta)) + \log(p(\theta)))$
	
	$p(\vec{y}|X,\vec{\theta}) = \prod\limits_{n=1}^N p(y_n\mid\vec{x}_n,\vec{\theta}) \Rightarrow TODO$
	
	%$p(y_n\mid\vec{x}_n,\vec{\theta}) \sim \mathcal{N}(y_n - \vec{\theta}^T\vec{x}, \sigma^2) \Rightarrow p(\vec{\theta}\mid\vec{x}_n) =  min_{\vec{\theta}} \dfrac{1}{2\sigma^2}\sum\limits_{n=1}^N (y_n - \vec{\theta}^T\vec{x}_n)^2 + \sum\limits_{n=1}^N \theta_n^2$
	$p(y_n\mid\vec{x}_n,\vec{\theta}) \sim \mathcal{N}(y_n - \vec{\theta}^T\vec{x}, \sigma^2) \Rightarrow \vec{\theta}^* = \min\limits_{\vec{\theta}} \dfrac{1}{2\sigma^2} \lVert\vec{y}-X\vec{\theta}\rVert_2^2 
	 + \lVert\vec{\theta}\rVert_2^2$
	
\end{document}

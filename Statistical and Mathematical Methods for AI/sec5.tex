\documentclass[11pt]{article}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{upgreek}
\usepackage{amssymb}
\usepackage[normalem]{ulem}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in
\begin{document}

% ========== Edit your name here
\author{Simone Montali - monta.li}
\title{SMM - Section 5 - Linear Systems}

\maketitle

\medskip
\section{Linear systems}
\subsection{Introduction to linear systems}
Now, using the matrix decompositions, we are able to solve linear systems. A linear system can be written as:
\begin{displaymath}
    Ax = b
\end{displaymath}
where $A$ is a mtrix of size $m \times n$, with $m\ge n$ and $x$ is a column vector of length $n$ and $b$ is a column vecotr of length $m$. This form can be expanded as a list of equations. We are interested in the existence/uniqueness of a solution, the numerical methods to find it (there can be more methods) and the conditioning of the problem. We are looking for two features: precision of the solution (related to an estimate of error, the numerical solution always has an error, therefore different algorithms produce different errors), and time of solution (an algorithm is efficient if it requires the lesser possible time). \\
We can now divide the solution in two categories: square and non-square systems.
\subsection{Square linear systems}
We now have a linear system
\begin{displaymath}
    Ax = b
\end{displaymath}
with $A$ of dimension $n \times n$, $x$ and $b$ of dimension $n$ exists and is unique iff one of the following conditions holds: 
\begin{itemize}
    \item A is non-singular
    \item rank$(A) = n$
    \item The system $Ax=0$ only admits the solution $x=0$
\end{itemize}
In principle, we can check that the solution is computable by dividing $b$ by $A$, thus multiplying $A^{-1}b$. The computation of the inverse of A is, sadly, computationally hard. So other solutions are needed. One of these is the \textit{Cramer's rule}, where each component $x_i$ of the solution is the ratio between $\frac{detA_i}{detA}$. This method is very costly too, therefore it is not widely used. The numerical methods follow two different approaches: the \textbf{direct methods} (which are more precise but computationally costly) and the \textbf{iterative methods} (which require a thoretically infinite number of steps, so they are less precise but faster).
\subsubsection{Direct methods}
These methods are based on the factorization of the matrix, like the LU factorization (computational cost $O\left(\frac{n^3}{3}\right)$). The solution is therefore computable by solving two triangular systems: $Ly=b$ and $Ux = y$. Why is this kind of complicated method used? Because the solution of a triangular system is much less expensive, and it is solvable by forward or backward substitution (which have computational cost of $O\left(\frac{n^2}{2}\right)$). In the case of the pivoting algorithm, we multiply both terms in $Ax=b$ by $P$, thus obtaining $PAx = Pb$ and then $Ly = Pb,Ux=y$.
\subsubsection{Iterative methods}
The basic idea, here, is constructing a sequence of vectors that converge to the exact solution.
\begin{displaymath}
    x^* = lim_{k\rightarrow \infty} x_k
\end{displaymath}
The convergence means that each component converges to a component of the exact solution. 
We've got to have a starting guess $x_0$. The sequence of vector is obtained by applying the same function (or set of operators) iteratively. 
One might ask: \textit{when can I stop the iteration?} This is the most critical part. \textbf{Stopping criteria} are the conditions that allow the machine to stop, and they're based on some quantity related to the residual, defined by $r_k = b-AX_k$ at iteration $k$. We can even base on the absolute criterion $||x_{k+1}-x_k||\le \tau$.
\subsubsection{Sparse matrices}
A sparse matrix is a matrix with a very low percentage of non-null elements, i.e. the majority of items are equal to $0$. These are important for data representation, since only the non-null elements are stored, saving a lot of memory.
\subsection{Inherent errors in linear systems}
Always remember that inherent errors are due to errors in the data representation and do not depend on the algorithm.
Even if we start with a really small representation error, we can have large output errors. We distinguish between well posed (where $|\Delta y| \approx |\Delta x|$) and ill posed (where $|\Delta y| >> |\Delta x|$) problems. In the second case, even with small representation errors, there's a tangible risk of great errors. Considering a linear system $Ax=b$, when $b$ slightly change we get $A(x+\Delta x) = b + \Delta b$. If now we wanted to compare $\Delta x$ with $\Delta b$, specifically their relative values $\left\|\frac{\Delta x}{x}\right\|$ and $\left\|\frac{\Delta b}{b}\right\|$.
\subsection{Linear least squares}
Considering an overdetermined system, finding the solution is impossible ($b$ does not lie on the subspace spanned by the columns of A), therefore we're looking for approximate solutions, specifically the best one. We then need some tools.

\subsubsection{Orthogonality and projections}
Since, when working in machine learning, visualizing data in a useful way can be pretty difficult, projections come in help. These compress the data while minimizing the loss of useful informations, therefore making data more \textit{densely informative}. 
% I started wandering in my thoughts here and lost part of the lesson, sorry about this lol
\subsection{Descent methods}
This is an iterative method for the solution of the minimization problem. These are the most widely used methods, that given the starting vector $x_0$, we compute the sequence of vectors with this formula:
\begin{displaymath}
    x_{k+1}=x_k+\alpha_kp_k
\end{displaymath}
where $p_k$ is a descent direction, and $\alpha$ is a step size. What happens is we add a step towards the direction.
\subsection{Gradient descent} 
The gradient descent is a first order optimization method where the descent direction is set as
\begin{displaymath}
    p_k = -\nabla f(x_k) 
\end{displaymath}
The convergence speed is linear, therefore this method is kind of slow. 
\subsubsection{Newton method}
It uses second order information of $f$, i.e. the Hessian.
The direction is set as $\mathbf{p}_{k}=-\mathrm{H}_{f}^{-1}\left(\mathbf{x}_{k}\right) \nabla f\left(\mathbf{x}_{k}\right)$
The convergence properties are related to the Hessian, and it is faster. At every iteration the speed increases. The computational cost is greater since we have to solve a linear system.
So, the cost per iteration is composed of the computing of the gradient PLUS the computing of the linear system solution. So, even though iterations are more complex, less of them are required. So there's a tradeoff between the complexity and the number of iterations.
There's not a best method, it depends on the cases.
There are some modifications in the Newton methods we could apply. The very difficult condition to hold is that $H$ has to be positive definite in every point we compute, so there are some relaxations to this condition. In some cases, $H$ is approximated:
\begin{displaymath}
    \mathrm{H}_{f}\left(\mathbf{x}_{k}\right) \simeq \mathrm{B}\left(\mathbf{x}_{k}\right)
\end{displaymath}
or we could only use the first derivatives.
A way of reducing the computational cost we can approximately solve the system: we compute the direction $p_k$ with iterative methods.
\subsection{Stochastic optimization}
When we're working with milions of data, the objective function (the function to minimize) is related to probabilistic events, having the form:
\begin{displaymath}
    G(\mathbf{x})=\sum_{i=1}^{n} G_{i}(\mathbf{x})
\end{displaymath}
Each $G_i$ is the so-called \textit{loss function}. For example, in a supervised classification problem we minimize the empirical risk function. In a standard gradient descent,
\begin{displaymath}
    
\begin{aligned}
    \mathbf{x}_{k+1} &=\mathbf{x}_{k}-\alpha_{k} \nabla G\left(\mathbf{x}_{k}\right) \\
    &=\mathbf{x}_{k}-\alpha_{k} \sum_{i=1}^{n} \nabla G_{i}\left(\mathbf{x}_{k}\right)
    \end{aligned}\end{displaymath}

When working with lots of data, the gradient computation is really expensive. In stochastic gradient descent, the true gradient is approximated at each iteration by the gradient of a single observation, randomly picked. The iteration step becomes:
\begin{displaymath}
    \mathbf{x}_{k+1}=\mathbf{x}_{k}-\alpha_{k} \nabla G_{i}\left(\mathbf{x}_{k}\right)
\end{displaymath}
We can compromise by computing the gradient on a small number of observations (\textit{mini-batch}).

\end{document}

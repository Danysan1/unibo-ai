# Association rules

Let's start with an example topic, the **market basket analysis**, proposed in 1992 just to introduce this chapter. 

*Given a set of commercial transactions, find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction.*

Some examples of association rules are <img src="svgs/12d271522bde8c84731f06d3ab9ee71d.svg?invert_in_darkmode" align=middle width=211.31793375pt height=24.65753399999998pt/> where the curly parenthesis specify sets, the arrow means *if the left is included, then the right is*.

We must be careful, we say *implication* but the meaning is *co-occurrence*, **not the logical implication**.

An association rule is an expression in the form <img src="svgs/ff73d5ba08026b41452e67b65cb89924.svg?invert_in_darkmode" align=middle width=50.82404249999998pt height=22.465723500000017pt/>, where <img src="svgs/7812d2f7a88b07d4ff5cf8e4376588dc.svg?invert_in_darkmode" align=middle width=147.91365104999997pt height=24.65753399999998pt/> and <img src="svgs/c0ddfd1e33ccc03bea5c505225cc7b50.svg?invert_in_darkmode" align=middle width=87.75532094999998pt height=24.65753399999998pt/> are itemsets. We need **rule evaluation metrics**: **support**, i.e. the fraction of the <img src="svgs/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode" align=middle width=14.999985000000004pt height=22.46574pt/> transactions that contain both <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.32879834999999pt height=22.465723500000017pt/> and <img src="svgs/9b325b9e31e85137d1de765f43c0f8bc.svg?invert_in_darkmode" align=middle width=12.92464304999999pt height=22.465723500000017pt/> (filter the transactions containing <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.32879834999999pt height=22.465723500000017pt/>, and count those that contain <img src="svgs/9b325b9e31e85137d1de765f43c0f8bc.svg?invert_in_darkmode" align=middle width=12.92464304999999pt height=22.465723500000017pt/>), and **confidence**, i.e. measuring how often all the items in <img src="svgs/9b325b9e31e85137d1de765f43c0f8bc.svg?invert_in_darkmode" align=middle width=12.92464304999999pt height=22.465723500000017pt/> appear in transactions that contain <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.32879834999999pt height=22.465723500000017pt/>.
<p align="center"><img src="svgs/9876f30a3859655da5b2a7618eff801a.svg?invert_in_darkmode" align=middle width=462.9111333pt height=38.83491479999999pt/></p>
Note that support is antimonotonic: when you add an item the support may only be equal or decrease.

Rules with low support can be generated by random associations, e.g. I buy champagne and diapers.

Rules with low confidence are not that reliable.

Now, our goal is, given a set of transactions, we want to find all the rules that satisfy some support/confidence thresholds. We could try a **brute-force approach**, listing all the possible association rules and computing support and confidence, then pruning rules that fail the thresholds, but this is **computationally prohibitive**.

## Frequent itemset generation

We have some strategies for this: reducing the number of candidates <img src="svgs/fb97d38bcc19230b0acd442e17db879c.svg?invert_in_darkmode" align=middle width=17.73973739999999pt height=22.465723500000017pt/> or reducing the number of comparisons <img src="svgs/8c33f867dad3095776e05cd8eb3836a1.svg?invert_in_darkmode" align=middle width=32.73970589999999pt height=22.465723500000017pt/>.

### Apriori principle

This says that if an itemset is frequent, then all of its subsets must also be frequent. This holds due to the following property of the support:
<p align="center"><img src="svgs/8c3a7201d441befe5b1a8df2e62ac830.svg?invert_in_darkmode" align=middle width=266.55976875pt height=16.438356pt/></p>
We use this to prune: the infrequent subsets are not useful.

We can then generate candidates, defining <img src="svgs/1a567506286617473a9c0d9b2172f951.svg?invert_in_darkmode" align=middle width=19.014878849999988pt height=22.465723500000017pt/> the candidate itemsets of size <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> and <img src="svgs/bfb6e556d3874a3157379133a8d7917a.svg?invert_in_darkmode" align=middle width=18.45327164999999pt height=22.465723500000017pt/> the frequent itemsets of size <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/>. Now, <img src="svgs/541e64a0f15941f48e12e43f9458509a.svg?invert_in_darkmode" align=middle width=73.45344599999999pt height=24.65753399999998pt/> is the set of subsets of <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.113876000000004pt height=14.155350000000013pt/> with <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> elements.

Now, we represent <img src="svgs/bfb6e556d3874a3157379133a8d7917a.svg?invert_in_darkmode" align=middle width=18.45327164999999pt height=22.465723500000017pt/> as a table with <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> columns where each row is a frequent itemset. We'll let the items in each row of <img src="svgs/bfb6e556d3874a3157379133a8d7917a.svg?invert_in_darkmode" align=middle width=18.45327164999999pt height=22.465723500000017pt/> be in alphabetical order (i.e. *lexicographic*), then, we can generate the candidates <img src="svgs/486b042b63ca15d29aa102127a8cb567.svg?invert_in_darkmode" align=middle width=35.658800099999986pt height=22.465723500000017pt/> by a self join on <img src="svgs/bfb6e556d3874a3157379133a8d7917a.svg?invert_in_darkmode" align=middle width=18.45327164999999pt height=22.465723500000017pt/>.

This is called a-priori because the computation is level-wise, and the evaluation at level <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> uses the **prior knowledge** acquired in the previous levels, to reduce the search space.

So, which factors affect the complexity? First of all, the **minimum support threshold** (lowering it results in a higher number of frequent itemsets, which may reduce pruning and increase the maximum length of itemsets): the number of complete reads is given by the maximum length of frequent itemsets+1. Secondly, the **dimensionality** of the dataset, which is proportional to the computational, memory and I/O costs.

So, there are a few ways of reducing the number of these frequent itemsets. It is useful to identify a significant subset of these:

- **Maximal Frequent Itemsets**, i.e. the smallest set of itemsets from which the frequent itemsets can be derived
- **Closed itemsets**, i.e. the minimal representation of itemsets without losing support information
  - <img src="svgs/e3e48dc87c5c913c44d94b0104bf57c2.svg?invert_in_darkmode" align=middle width=53.67565334999998pt height=22.465723500000017pt/> Is redundant if there exists <img src="svgs/8dcd0c4ea2d87fe2ad1771c5b4c64f13.svg?invert_in_darkmode" align=middle width=62.07748304999999pt height=24.7161288pt/> such that the support and confidence are the same, with the prime being subsets of the normal sets. 
  - There are lots of algorithms to compute these

A *MFI* does not have any frequent immediate supersets. The MFI are near the border dividing frequent by not frequent itemsets in the lattice.

## Confidence

The **confidence** can be **computed from the supports!**
<p align="center"><img src="svgs/ed6802aa4316485e447b50e60a38e8a7.svg?invert_in_darkmode" align=middle width=206.17592985pt height=38.83491479999999pt/></p>

# Association rules

Let's start with an example topic, the **market basket analysis**, proposed in 1992 just to introduce this chapter. 

*Given a set of commercial transactions, find rules that will predict the occurrence of an item based on the occurrences of other items in the transaction.*

Some examples of association rules are $\{bread, milk\}\rightarrow\{coke,eggs\}$ where the curly parenthesis specify sets, the arrow means *if the left is included, then the right is*.

We must be careful, we say *implication* but the meaning is *co-occurrence*, **not the logical implication**.

An association rule is an expression in the form $A\Rightarrow C$, where $A=\{Milk, Diaper\}$ and $C=\{Beer\}$ are itemsets. We need **rule evaluation metrics**: **support**, i.e. the fraction of the $N$ transactions that contain both $A$ and $C$ (filter the transactions containing $A$, and count those that contain $C$), and **confidence**, i.e. measuring how often all the items in $C$ appear in transactions that contain $A$.
$$
\text{sup}=\frac{\sigma(Milk, Diaper, Beer)}{N}\hspace{20px}\text{conf}=\frac{\sigma(Milk, Diaper, Beer)}{\sigma(Milk, Diaper)}
$$
Note that support is antimonotonic: when you add an item the support may only be equal or decrease.

Rules with low support can be generated by random associations, e.g. I buy champagne and diapers.

Rules with low confidence are not that reliable.

Now, our goal is, given a set of transactions, we want to find all the rules that satisfy some support/confidence thresholds. We could try a **brute-force approach**, listing all the possible association rules and computing support and confidence, then pruning rules that fail the thresholds, but this is **computationally prohibitive**.

## Frequent itemset generation

We have some strategies for this: reducing the number of candidates $M$ or reducing the number of comparisons $NM$.

### Apriori principle

This says that if an itemset is frequent, then all of its subsets must also be frequent. This holds due to the following property of the support:
$$
\forall X,Y: (X\subseteq Y)\Rightarrow sup(x)\ge sup(Y)
$$
We use this to prune: the infrequent subsets are not useful.

We can then generate candidates, defining $C_k$ the candidate itemsets of size $k$ and $L_k$ the frequent itemsets of size $k$. Now, $subset_k(c)$ is the set of subsets of $c$ with $k$ elements.

Now, we represent $L_k$ as a table with $k$ columns where each row is a frequent itemset. We'll let the items in each row of $L_k$ be in alphabetical order (i.e. *lexicographic*), then, we can generate the candidates $C_{k+1}$ by a self join on $L_k$.

This is called a-priori because the computation is level-wise, and the evaluation at level $k$ uses the **prior knowledge** acquired in the previous levels, to reduce the search space.

So, which factors affect the complexity? First of all, the **minimum support threshold** (lowering it results in a higher number of frequent itemsets, which may reduce pruning and increase the maximum length of itemsets): the number of complete reads is given by the maximum length of frequent itemsets+1. Secondly, the **dimensionality** of the dataset, which is proportional to the computational, memory and I/O costs.

So, there are a few ways of reducing the number of these frequent itemsets. It is useful to identify a significant subset of these:

- **Maximal Frequent Itemsets**, i.e. the smallest set of itemsets from which the frequent itemsets can be derived
- **Closed itemsets**, i.e. the minimal representation of itemsets without losing support information
  - $X\rightarrow Y$ Is redundant if there exists $X'\rightarrow Y'$ such that the support and confidence are the same, with the prime being subsets of the normal sets. 
  - There are lots of algorithms to compute these

A *MFI* does not have any frequent immediate supersets. The MFI are near the border dividing frequent by not frequent itemsets in the lattice.

## Confidence

The **confidence** can be **computed from the supports!**
$$
\text{conf}(A\Rightarrow C) = \frac{sup(A\Rightarrow C)}{sup(A)}
$$

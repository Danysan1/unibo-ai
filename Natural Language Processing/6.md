# 6. Vector semantics and embeddings - part I: sparse representation
As the affective meaning can be represented by defining _n_ different dimensions and representing words as positions in this _n_-dimensional space, also **word meaning can be represented as a point in a multidimensional space**, using the so called **vector semantics models**.

The core idea behind vector semantics is that words are defined by their usage, this means that a word is defined by its environment or _distribution_ in language use, i.e. the set of contexts in which it occurs.

Vectors to represent words are called **embeddings** and it is the standard way to represent meaning in NLP.

Two common models of embeddings are **TF-IDF** and **Word2vec**. The former use sparse vector, the latter dense vectors.

## Words and Vectors

A **co-occurrence matrix** represents how often words co-occur.

## Reweighting

Frequency is of course useful, however overly frequent words (like stopwords) are not very informative about the context. So raw frequency results to be a bad representation, because it is skewed and not very discriminative.

The goals of reweighting is to amplify what is important, trustworthy and unusual, while deemphasizing what is mundane and quirky, moving away from raw counts.

The **term frequency** (tf) is defined as the log-transformed frequency count:

tf<sub>_t,d_</sub> = log<sub>10</sub>(_Count(t,d)_ + 1)

**Inverse document frequency** (idf) is defined as

idf<sub>_t_</sub> = log<sub>10</sub> (_N_ / df<sub>_t_</sub>)

where df<sub>_t_</sub> is the number of documents where _t_ occurs.

Words that occur many times in a document have a high tf, but if they also appear in many different documents they will have a low idf, like _the_ or _good_.

Our metric is **tf-idf**<sub>_t,d_</sub> = tf<sub>t,d</sub> &times; idf<sub>_t_</sub>

AN alternative to tf-idf is positive pointwise mutual information, which is defined as

PPMI(w,c) = _max_(PMI(w,c), 0)

where PMI is the pointwise mutual information PMI(w,c) = log<sub>2</sub> (P(w,c) / P(w)P(c))
